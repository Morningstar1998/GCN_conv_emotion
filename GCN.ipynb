{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Morningstar1998/GCN_conv_emotion/blob/main/GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buT_FRRBCQd8"
      },
      "source": [
        "Clonamos el repositorio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiTLaPfZ-i_r",
        "outputId": "da9cf56c-260a-4dba-c98d-318484400f66"
      },
      "source": [
        "!git clone https://github.com/Morningstar1998/GCN_conv_emotion"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GCN_conv_emotion'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 66 (delta 20), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (66/66), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdrRWKenCUlk"
      },
      "source": [
        "Instalamos las dependencias necesarias para colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCi9b5TRCMjI",
        "outputId": "fe8c5ab0-33b0-478d-d50c-640ed20480d1"
      },
      "source": [
        "!pip install torch-scatter #-f https://pytorch-geometric.com/whl/torch-1.8.1+cu101.html\n",
        "!pip install torch-sparse #-f https://pytorch-geometric.com/whl/torch-1.8.1+cu101.html\n",
        "!pip install torch-cluster #-f #https://pytorch-geometric.com/whl/torch-1.8.1+cu101.html\n",
        "!pip install torch-spline-conv #-f https://pytorch-geometric.com/whl/torch-1.8.1+cu101.html\n",
        "!pip install torch-geometric\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch-scatter\n",
            "  Downloading https://files.pythonhosted.org/packages/91/26/f2f0767a0e72c38d5d17f53117deb8aaafaf58f4ad658cee56cd5158c979/torch_scatter-2.0.6.tar.gz\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.0.6-cp37-cp37m-linux_x86_64.whl size=292559 sha256=616d7203be1baf351b56438dc7cdf81460f9956cdc2bd30a4ade48f3f907f485\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/bf/04/7c9e0c1466d37548e5c7c7c2daee2b7f90ce616f102c68bf07\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.6\n",
            "Collecting torch-sparse\n",
            "  Downloading https://files.pythonhosted.org/packages/9a/86/699eb78ea7ce259da7f9953858ec2a064e4e5f5e6bf7de53aa6c8bb8b9a8/torch_sparse-0.6.9.tar.gz\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Building wheels for collected packages: torch-sparse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5w7oeAPDHSH"
      },
      "source": [
        "Anexamos los .py donde se encuentra el modelo a la libreria"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt0UmV7gCZsr"
      },
      "source": [
        "import sys\n",
        "lib='/content/GCN_conv_emotion/GCN'\n",
        "sys.path.insert(0,lib)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t8cP_49Catk"
      },
      "source": [
        "Importamos las librerias "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtrOCbHZJ4vd"
      },
      "source": [
        "import model\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from dataloader import IEMOCAPDataset\n",
        "from model import MaskedNLLLoss, LSTMModel, GRUModel, DialogRNNModel, DialogueGCNModel\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report, precision_recall_fscore_support\n",
        "import pickle\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.vocab import GloVe\n",
        "from functools import reduce\n",
        "import torch as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abNE3_r-DLNP"
      },
      "source": [
        "Para usar un modelo especifico sobre escribir el path con el modelo deseado que se encuente en el github o de manera local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaFRuKNfC8bf"
      },
      "source": [
        "path='/content/GCN_conv_emotion/models/GNCGraphDialogRNN_4.pth'\n",
        "model = torch.load(path)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy86wHuKDh-X"
      },
      "source": [
        "Esto es para extender lo que se puede ver con colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3tTdtj2Dgu4"
      },
      "source": [
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.describe_option('max_colwidth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj7IyHDIDorw"
      },
      "source": [
        "Visualizamos el pickle con los features guardados "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34xl8x2nDo0Z"
      },
      "source": [
        "x=pickle.load(open('/content/GCN_conv_emotion/GCN/IEMOCAP_features/IEMOCAP_features.pkl', 'rb'), encoding='latin1')\n",
        "x[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6SCse20DxXY"
      },
      "source": [
        "Vemos como funciona glove (El cual ya esta tanto importado pero falta instalarlo) Para esta version usaremos el que da 300 como dimension\n",
        "\n",
        "\n",
        "embedding_glove = GloVe(name='840B', dim=300)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x1vrv-1KfFR"
      },
      "source": [
        "embedding_glove = GloVe(name='840B', dim=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDHU10DPEHRB"
      },
      "source": [
        "print(embedding_glove['beatiful'].size())\n",
        "print(embedding_glove['beatiful'])\n",
        "c=embedding_glove['beatiful']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjn11yB5DzhQ"
      },
      "source": [
        "Tokenizamos y pasamos a un tensor usando GloVe y la oracion para despues concatenarla en un solo tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7aY64f1EP1f"
      },
      "source": [
        "text = \"Hello my friends! How are you doing today?\"\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "text_token=tokenizer(text)\n",
        "data=[]\n",
        "for x in text_token:\n",
        "  data.append(embedding_glove[x])\n",
        "print(data[1].size())\n",
        "y=torch.empty \n",
        "datafinal=reduce(lambda x,y: T.cat((x,y)), data[:-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj4JECKWEhAZ"
      },
      "source": [
        "datafinal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz1jv6OOEkLR"
      },
      "source": [
        "Dado que el modelo de prediccion no funciono puesto que no explican de manera contundente como realizar una inferencia fuera del set de datos. Trataremos de bypasearlo . El modelo pide 3 datos adicionales al input umask , qmask y lenght. Analizado el tipo de datos...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaAeXUIUE8Dp"
      },
      "source": [
        "print(datafinal.size())\n",
        "umask=torch.tensor([0,1])\n",
        "qmask=torch.tensor([1,0])\n",
        "l=[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKb1RlpYFAzP"
      },
      "source": [
        "output = model(datafinal,umask,qmask)\n",
        "_, predicted = torch.max(output, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2C0EjvDJE1B"
      },
      "source": [
        "# TRAINING\n",
        "\n",
        "---\n",
        "Para entrenar los modelos y ver su desempeño basta con dar correr la celda. Si se desea correr el modelo y guardarlo se necesita ingresar al /content GCN_conv_emotion/GCN/train_IEMOCAP.py y descomentar las lineas 348 si se usa convolucion grafica o la 362 si se usa solo el modelo base en caso de que no se requiera guardar comentar dichas lineas evitara que se guarde.\n",
        "\n",
        "---\n",
        "\n",
        "El repositorio cuenta con 4 modelos base: GRU , Linear , LTSM y RNN\n",
        "\n",
        "Se puede combinar con una convulsion grafica\n",
        "\n",
        "---\n",
        "\n",
        "Los mejores resultados se obtienen con epochs entre 50 y 60. No se puede usar GPU por cuestiones de colab\n",
        "\n",
        "---\n",
        "De manera adicional, si no se ejecuta en colab sera necesario cambiar  la linea 11 del dataloader.py (GCN_conv_emotion/GCN/dataloader.py) y cambiar el archivo pickle por su nueva direccion (el archivo se encuentra GCN_conv_emotion/GCN/IEMOCAP_features) y la linea 338 y 352 del train_IEMOCAP.py (GCN_conv_emotion/GCN/train_IEMOCAP.py) por un path nuevo donde se se guste guardar los modelos nuevos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfEEk6H1F5JK"
      },
      "source": [
        "!python /content/GCN_conv_emotion/GCN/train_IEMOCAP.py --active-listener --epoch 5 --base-model 'GRU' --nodal-attention --dropout 0.4 --lr 0.0003 --batch-size 32 --class-weight --l2 0.0 --no-cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDxEy8pyHfGg"
      },
      "source": [
        "!python /content/GCN_conv_emotion/GCN/train_IEMOCAP.py --active-listener --epoch 5 --base-model 'DialogRNN' --nodal-attention --dropout 0.4 --lr 0.0003 --batch-size 32 --class-weight --l2 0.0 --no-cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EZ_jDbGHfNn"
      },
      "source": [
        "!python /content/GCN_conv_emotion/GCN/train_IEMOCAP.py --active-listener --epoch 5 --base-model 'LSTM' --nodal-attention --dropout 0.4 --lr 0.0003 --batch-size 32 --class-weight --l2 0.0 --no-cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqTwVqi4HfQj"
      },
      "source": [
        "!python /content/GCN_conv_emotion/GCN/train_IEMOCAP.py --epoch 5 --base-model 'LSTM' --graph-model --nodal-attention --dropout 0.4 --lr 0.0003 --batch-size 32 --class-weight --l2 0.0 --no-cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDHf4rgnHfVA"
      },
      "source": [
        "!python /content/GCN_conv_emotion/GCN/train_IEMOCAP.py --epoch 5 --base-model 'DialogRNN' --graph-model --nodal-attention --dropout 0.4 --lr 0.0003 --batch-size 32 --class-weight --l2 0.0 --no-cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZEtVvrjHfXp"
      },
      "source": [
        "!python /content/GCN_conv_emotion/GCN/train_IEMOCAP.py --epoch 5 --base-model 'GRU' --graph-model --nodal-attention --dropout 0.4 --lr 0.0003 --batch-size 32 --class-weight --l2 0.0 --no-cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWF6vjS5Hfag"
      },
      "source": [
        "!python /content/GCN_conv_emotion/GCN/train_IEMOCAP.py --epoch 5 --base-model 'None' --graph-model --nodal-attention --dropout 0.4 --lr 0.0003 --batch-size 32 --class-weight --l2 0.0 --no-cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phq0_taAHfc5"
      },
      "source": [
        "#Entrenamiento original, con este se obtienen los resultados del paper\n",
        "!python /content/GCN_conv_emotion/GCN/train_IEMOCAP.py --base-model 'LSTM' --graph-model --nodal-attention --dropout 0.4 --lr 0.0003 --batch-size 32 --class-weight --l2 0.0 --no-cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COB8fvztWPKh"
      },
      "source": [
        "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation. D. Ghosal, N. Majumder, S. Poria, N. Chhaya, & A. Gelbukh. EMNLP-IJCNLP (2019), Hong Kong, China."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ4e3TY8aqRp"
      },
      "source": [
        "# GITHUB ORIGINAL\n",
        "\n",
        "---\n",
        "https://github.com/declare-lab/conv-emotion/tree/f34f92eea415465041f7d3f7f8454f88ee9f3acc#dialoguegcn-a-graph-convolutional-neural-network-for-emotion-recognition-in-conversation\n"
      ]
    }
  ]
}